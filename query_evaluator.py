# -*- coding: utf-8 -*-
"""
QGpT: Improving Table Retrieval with Question Generation from Partial Tables
Query Evaluator

This script evaluates query performance using test queries against built vector databases.
It supports both single query testing and batch evaluation with ground truth comparison.

Usage:
    python query_evaluator.py "query text" --db database.db
    python query_evaluator.py --test-file test_queries.json --db database.db
    python query_evaluator.py --batch-eval  # æ‰¹æ¬¡è©•ä¼°æ‰€æœ‰æ¸¬è©¦é›†

Author: QGpT Research Team
Repository: https://github.com/UDICatNCHU/QGpT
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from pymilvus import MilvusClient, model

from utils import (
    load_json_dataset,
    format_search_results,
    get_corpus_files,
    extract_corpus_name_from_path,
    generate_db_name,
    generate_collection_name
)


class QGpTQueryEvaluator:
    """QGpT æŸ¥è©¢è©•ä¼°å™¨"""
    
    def __init__(self, db_path: str, collection_name: str):
        """
        åˆå§‹åŒ–æŸ¥è©¢è©•ä¼°å™¨
        
        Args:
            db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘
            collection_name: å‘é‡é›†åˆåç¨±
        """
        self.db_path = db_path
        self.collection_name = collection_name
        self.client = None
        self.embedding_fn = None
        self.initialize()
    
    def initialize(self):
        """åˆå§‹åŒ– Milvus å®¢æˆ¶ç«¯å’ŒåµŒå…¥å‡½æ•¸"""
        try:
            if not Path(self.db_path).exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™åº«æª”æ¡ˆ: {self.db_path}")
            
            self.client = MilvusClient(self.db_path)
            self.embedding_fn = model.DefaultEmbeddingFunction()
            
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(collection_name=self.collection_name):
                raise ValueError(f"æ‰¾ä¸åˆ°é›†åˆ '{self.collection_name}' åœ¨è³‡æ–™åº« '{self.db_path}'")
            
            print(f"âœ… æˆåŠŸé€£æ¥åˆ°è³‡æ–™åº«: {self.db_path}")
            print(f"âœ… ä½¿ç”¨é›†åˆ: {self.collection_name}")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
            sys.exit(1)
    
    def search(self, query: str, limit: int = 5) -> List[Dict]:
        """
        åŸ·è¡Œæœç´¢æŸ¥è©¢
        
        Args:
            query: æœç´¢æŸ¥è©¢å­—ç¬¦ä¸²
            limit: è¿”å›çµæœçš„æ•¸é‡
            
        Returns:
            æœç´¢çµæœåˆ—è¡¨
        """
        try:
            # å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡
            query_vector = self.embedding_fn.encode_queries([query])
            
            # åŸ·è¡Œæœç´¢
            search_results = self.client.search(
                collection_name=self.collection_name,
                data=query_vector,
                limit=limit,
                output_fields=["text", "filename", "sheet_name", "original_id"]
            )
            
            # æ ¼å¼åŒ–çµæœ
            formatted_results = []
            for result in search_results[0]:
                formatted_results.append({
                    'score': 1 - result['distance'],  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                    'distance': result['distance'],
                    'filename': result['entity']['filename'],
                    'sheet_name': result['entity']['sheet_name'],
                    'original_id': result['entity']['original_id'],
                    'text': result['entity']['text']
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ æœç´¢éŒ¯èª¤: {e}")
            return []
    
    def evaluate_single_query(self, query: str, ground_truth: Optional[List[str]] = None, 
                            limit: int = 5) -> Dict:
        """
        è©•ä¼°å–®ä¸€æŸ¥è©¢
        
        Args:
            query: æŸ¥è©¢å­—ç¬¦ä¸²
            ground_truth: æ­£ç¢ºç­”æ¡ˆåˆ—è¡¨ï¼ˆæª”æ¡ˆåæˆ–IDï¼‰
            limit: è¿”å›çµæœæ•¸é‡
            
        Returns:
            è©•ä¼°çµæœ
        """
        results = self.search(query, limit)
        
        evaluation = {
            'query': query,
            'results_count': len(results),
            'results': results
        }
        
        # å¦‚æœæœ‰æ­£ç¢ºç­”æ¡ˆï¼Œè¨ˆç®—æº–ç¢ºç‡æŒ‡æ¨™
        if ground_truth:
            retrieved_ids = [r['original_id'] for r in results]
            retrieved_files = [r['filename'] for r in results]
            
            # è¨ˆç®—å‘½ä¸­ç‡ï¼ˆHit Rateï¼‰
            hits_by_id = sum(1 for gt in ground_truth if gt in retrieved_ids)
            hits_by_file = sum(1 for gt in ground_truth if gt in retrieved_files)
            
            evaluation.update({
                'ground_truth': ground_truth,
                'hits_by_id': hits_by_id,
                'hits_by_file': hits_by_file,
                'hit_rate_by_id': hits_by_id / len(ground_truth) if ground_truth else 0,
                'hit_rate_by_file': hits_by_file / len(ground_truth) if ground_truth else 0,
                'precision_at_k': hits_by_id / len(results) if results else 0
            })
        
        return evaluation


class BatchEvaluator:
    """æ‰¹æ¬¡è©•ä¼°å™¨"""
    
    def __init__(self):
        self.test_files_dir = "Test_Query_and_GroundTruth_Table"
    
    def get_test_files(self) -> List[str]:
        """å–å¾—æ‰€æœ‰æ¸¬è©¦æª”æ¡ˆ"""
        test_dir = Path(self.test_files_dir)
        if not test_dir.exists():
            return []
        
        return [str(f) for f in test_dir.glob("*.json")]
    
    def match_corpus_to_test(self, test_file: str) -> Optional[str]:
        """
        å°‡æ¸¬è©¦æª”æ¡ˆåŒ¹é…åˆ°å°æ‡‰çš„èªæ–™åº«
        
        Args:
            test_file: æ¸¬è©¦æª”æ¡ˆè·¯å¾‘
            
        Returns:
            å°æ‡‰çš„èªæ–™åº«è³‡æ–™åº«è·¯å¾‘ï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å› None
        """
        test_name = Path(test_file).stem
        
        # å®šç¾©æ¸¬è©¦æª”æ¡ˆåˆ°èªæ–™åº«çš„æ˜ å°„è¦å‰‡
        mapping_rules = {
            'MiMoTable-English': 'Table1_mimo_table_length_variation_mimo_en',
            'MiMoTable-Chinese': 'Table1_mimo_table_length_variation_mimo_ch',
            'E2E-WTQ': 'Table5_Single_Table_Retrieval_QGpT',
            'FetaQA': 'Table5_Single_Table_Retrieval_QGpT',
            'OTT-QA': 'Table7_OTTQA',
            'MMQA-2tables': 'Table6_Multi_Table_Retrieval_2_tables',
            'MMQA-3tables': 'Table6_Multi_Table_Retrieval_3_tables'
        }
        
        # å°‹æ‰¾åŒ¹é…çš„èªæ–™åº«
        for test_key, corpus_pattern in mapping_rules.items():
            if test_key in test_name:
                # å°‹æ‰¾å°æ‡‰çš„è³‡æ–™åº«æª”æ¡ˆ
                corpus_files = get_corpus_files()
                for corpus_info in corpus_files:
                    if corpus_pattern in corpus_info['name']:
                        db_path = corpus_info['db_name']
                        if Path(db_path).exists():
                            return db_path
        
        return None
    
    def evaluate_test_file(self, test_file: str, db_path: str, limit: int = 5) -> Dict:
        """
        è©•ä¼°æ¸¬è©¦æª”æ¡ˆ
        
        Args:
            test_file: æ¸¬è©¦æª”æ¡ˆè·¯å¾‘
            db_path: è³‡æ–™åº«è·¯å¾‘
            limit: æ¯å€‹æŸ¥è©¢è¿”å›çš„çµæœæ•¸é‡
            
        Returns:
            è©•ä¼°çµæœ
        """
        # è¼‰å…¥æ¸¬è©¦è³‡æ–™
        test_data = load_json_dataset(test_file)
        
        # æ ¹æ“šè³‡æ–™åº«è·¯å¾‘ç”Ÿæˆé›†åˆåç¨±
        corpus_name = Path(db_path).stem.replace('qgpt_', '')
        collection_name = generate_collection_name(corpus_name)
        
        # åˆå§‹åŒ–è©•ä¼°å™¨
        evaluator = QGpTQueryEvaluator(db_path, collection_name)
        
        results = []
        total_hit_rate = 0
        total_precision = 0
        
        print(f"ğŸ”„ è©•ä¼°æ¸¬è©¦æª”æ¡ˆ: {Path(test_file).name}")
        print(f"   æŸ¥è©¢æ•¸é‡: {len(test_data)}")
        
        for i, item in enumerate(test_data):
            query = item.get('question', '')
            
            # æå–æ­£ç¢ºç­”æ¡ˆï¼ˆæ ¹æ“šæ¸¬è©¦æª”æ¡ˆçµæ§‹èª¿æ•´ï¼‰
            ground_truth = []
            if 'spreadsheet_list' in item:
                ground_truth = item['spreadsheet_list']
            elif 'answer' in item:
                # æŸäº›æ¸¬è©¦æª”æ¡ˆå¯èƒ½æœ‰ä¸åŒçš„çµæ§‹
                pass
            
            # åŸ·è¡Œè©•ä¼°
            eval_result = evaluator.evaluate_single_query(query, ground_truth, limit)
            results.append(eval_result)
            
            # ç´¯è¨ˆæŒ‡æ¨™
            if 'hit_rate_by_id' in eval_result:
                total_hit_rate += eval_result['hit_rate_by_id']
            if 'precision_at_k' in eval_result:
                total_precision += eval_result['precision_at_k']
            
            # é¡¯ç¤ºé€²åº¦
            if (i + 1) % 10 == 0:
                print(f"   è™•ç†é€²åº¦: {i + 1}/{len(test_data)}")
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_hit_rate = total_hit_rate / len(test_data) if test_data else 0
        avg_precision = total_precision / len(test_data) if test_data else 0
        
        return {
            'test_file': test_file,
            'db_path': db_path,
            'total_queries': len(test_data),
            'avg_hit_rate': avg_hit_rate,
            'avg_precision': avg_precision,
            'detailed_results': results
        }
    
    def run_batch_evaluation(self, limit: int = 5, save_results: bool = True) -> Dict:
        """
        åŸ·è¡Œæ‰¹æ¬¡è©•ä¼°
        
        Args:
            limit: æ¯å€‹æŸ¥è©¢è¿”å›çš„çµæœæ•¸é‡
            save_results: æ˜¯å¦å„²å­˜è©³ç´°çµæœ
            
        Returns:
            æ‰¹æ¬¡è©•ä¼°çµæœ
        """
        test_files = self.get_test_files()
        if not test_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æ¸¬è©¦æª”æ¡ˆ")
            return {}
        
        batch_results = {}
        
        print(f"ğŸ”„ é–‹å§‹æ‰¹æ¬¡è©•ä¼°ï¼Œæ‰¾åˆ° {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆ")
        
        for test_file in test_files:
            print(f"\n{'='*60}")
            
            # å°‹æ‰¾å°æ‡‰çš„èªæ–™åº«è³‡æ–™åº«
            db_path = self.match_corpus_to_test(test_file)
            if not db_path:
                print(f"âš ï¸  è·³é {Path(test_file).name}ï¼šæ‰¾ä¸åˆ°å°æ‡‰çš„èªæ–™åº«è³‡æ–™åº«")
                continue
            
            try:
                # åŸ·è¡Œè©•ä¼°
                result = self.evaluate_test_file(test_file, db_path, limit)
                batch_results[Path(test_file).stem] = result
                
                print(f"âœ… å®Œæˆè©•ä¼°: {Path(test_file).name}")
                print(f"   å¹³å‡å‘½ä¸­ç‡: {result['avg_hit_rate']:.4f}")
                print(f"   å¹³å‡ç²¾ç¢ºç‡: {result['avg_precision']:.4f}")
                
            except Exception as e:
                print(f"âŒ è©•ä¼°å¤±æ•—: {Path(test_file).name} - {e}")
        
        # å„²å­˜çµæœ
        if save_results and batch_results:
            results_file = f"batch_evaluation_results_top{limit}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(batch_results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ è©•ä¼°çµæœå·²å„²å­˜åˆ°: {results_file}")
        
        return batch_results


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description='QGpT æŸ¥è©¢è©•ä¼°å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # å–®ä¸€æŸ¥è©¢æ¸¬è©¦
  python query_evaluator.py "è²¡å‹™å ±è¡¨" --db qgpt_Table1_mimo_ch.db
  
  # ä½¿ç”¨æ¸¬è©¦æª”æ¡ˆè©•ä¼°
  python query_evaluator.py --test-file Test_Query_and_GroundTruth_Table/MiMoTable-English_test.json --db qgpt_Table1_mimo_en.db
  
  # æ‰¹æ¬¡è©•ä¼°æ‰€æœ‰æ¸¬è©¦é›†
  python query_evaluator.py --batch-eval
  
  # æ‰¹æ¬¡è©•ä¼°ä¸¦æŒ‡å®šè¿”å›çµæœæ•¸é‡
  python query_evaluator.py --batch-eval --limit 10
        """
    )
    
    parser.add_argument('query', nargs='?', help='æœç´¢æŸ¥è©¢å­—ç¬¦ä¸²')
    parser.add_argument('--db', help='å‘é‡è³‡æ–™åº«è·¯å¾‘')
    parser.add_argument('--collection', help='å‘é‡é›†åˆåç¨±ï¼ˆè‡ªå‹•å¾è³‡æ–™åº«åç¨±æ¨å°ï¼‰')
    parser.add_argument('--test-file', help='æ¸¬è©¦æŸ¥è©¢æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--batch-eval', action='store_true', help='æ‰¹æ¬¡è©•ä¼°æ‰€æœ‰æ¸¬è©¦é›†')
    parser.add_argument('--limit', type=int, default=5, help='è¿”å›çµæœæ•¸é‡ (é è¨­: 5)')
    parser.add_argument('--format', choices=['detailed', 'simple', 'json'], 
                       default='detailed', help='è¼¸å‡ºæ ¼å¼ (é è¨­: detailed)')
    parser.add_argument('--save', action='store_true', help='å„²å­˜è©•ä¼°çµæœåˆ°æª”æ¡ˆ')
    
    args = parser.parse_args()
    
    # æ‰¹æ¬¡è©•ä¼°
    if args.batch_eval:
        evaluator = BatchEvaluator()
        results = evaluator.run_batch_evaluation(limit=args.limit, save_results=args.save)
        
        # é¡¯ç¤ºç¸½çµ
        if results:
            print(f"\n{'='*60}")
            print("æ‰¹æ¬¡è©•ä¼°ç¸½çµ:")
            for test_name, result in results.items():
                print(f"  {test_name}:")
                print(f"    æŸ¥è©¢æ•¸é‡: {result['total_queries']}")
                print(f"    å¹³å‡å‘½ä¸­ç‡: {result['avg_hit_rate']:.4f}")
                print(f"    å¹³å‡ç²¾ç¢ºç‡: {result['avg_precision']:.4f}")
        
        return
    
    # å–®ä¸€æŸ¥è©¢æˆ–æ¸¬è©¦æª”æ¡ˆè©•ä¼°
    if not args.db:
        print("âŒ è«‹æä¾›è³‡æ–™åº«è·¯å¾‘ (--db)")
        sys.exit(1)
    
    # è‡ªå‹•ç”Ÿæˆé›†åˆåç¨±
    if not args.collection:
        corpus_name = Path(args.db).stem.replace('qgpt_', '')
        collection_name = generate_collection_name(corpus_name)
    else:
        collection_name = args.collection
    
    # åˆå§‹åŒ–è©•ä¼°å™¨
    evaluator = QGpTQueryEvaluator(args.db, collection_name)
    
    # æ¸¬è©¦æª”æ¡ˆè©•ä¼°
    if args.test_file:
        if not Path(args.test_file).exists():
            print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦æª”æ¡ˆ: {args.test_file}")
            sys.exit(1)
        
        batch_evaluator = BatchEvaluator()
        result = batch_evaluator.evaluate_test_file(args.test_file, args.db, args.limit)
        
        print(f"\nè©•ä¼°çµæœ:")
        print(f"æ¸¬è©¦æª”æ¡ˆ: {result['test_file']}")
        print(f"æŸ¥è©¢ç¸½æ•¸: {result['total_queries']}")
        print(f"å¹³å‡å‘½ä¸­ç‡: {result['avg_hit_rate']:.4f}")
        print(f"å¹³å‡ç²¾ç¢ºç‡: {result['avg_precision']:.4f}")
        
        if args.save:
            results_file = f"evaluation_{Path(args.test_file).stem}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"è©³ç´°çµæœå·²å„²å­˜åˆ°: {results_file}")
        
        return
    
    # å–®ä¸€æŸ¥è©¢
    if args.query:
        results = evaluator.search(args.query, args.limit)
        output = format_search_results(results, args.query, args.format)
        print(output)
        return
    
    # å¦‚æœæ²’æœ‰æä¾›åƒæ•¸ï¼Œé¡¯ç¤ºå¹«åŠ©
    parser.print_help()


if __name__ == "__main__":
    main()
